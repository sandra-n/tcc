{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_CnjGOo8R1p"
      },
      "source": [
        "# Imports and packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iTfnXbs0301B"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\sandr\\anaconda3\\envs\\tccenv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.nn as nn\n",
        "from torchinfo import summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "CUDA_VISIBLE_DEVICES=\"\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXGZ5w4mA1p8"
      },
      "source": [
        "# Dataprep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "m8_5WeZ_A4Qb"
      },
      "outputs": [],
      "source": [
        "# tirar @ e links\n",
        "\n",
        "def removeMention(tweet):\n",
        "  words = tweet.split()\n",
        "  words = [word for word in words if \"@\" not in word ]\n",
        "  newTweet = ' '.join(words)\n",
        "  return newTweet\n",
        "\n",
        "  # substituir menção por <USER>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "kj7ITGhuW-fw"
      },
      "outputs": [],
      "source": [
        "import emoji\n",
        "from emoji import UNICODE_EMOJI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vr3YqJy2HBTh"
      },
      "outputs": [],
      "source": [
        "# search your emoji\n",
        "def is_emoji(s, language=\"en\"):\n",
        "    return s in UNICODE_EMOJI[language]\n",
        "\n",
        "# add space near your emoji\n",
        "def add_space(text):\n",
        "    return ''.join(' ' + char + ' ' if is_emoji(char) else char for char in text).strip()\n",
        "\n",
        "def separateEmoji(tweet):\n",
        "  return add_space(tweet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "blaHPJEGD2KK"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "regex = re.compile(\"(http://t\\.co.{12})|(https://t\\.co.{11})\")\n",
        "\n",
        "def removeLink(tweet):\n",
        "  #words = tweet.split()\n",
        "  #links = [word for word in words if \"t.co\" in word]\n",
        "  words = regex.sub('',tweet)\n",
        "  return words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orC9lkWEXoBL"
      },
      "source": [
        "# LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zheMpxKPU0qm"
      },
      "source": [
        "# Bag of words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "KUV8oorsU262"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "\n",
        "def tweets_vocab(dataset, n=None):\n",
        "    if n is None:\n",
        "        n = len(dataset)\n",
        "    all_tweets = []\n",
        "    for tweet in dataset:\n",
        "        words = tweet.split()\n",
        "        all_tweets.append(words)\n",
        "\n",
        "    all_tweets = [item for sublist in all_tweets for item in sublist]\n",
        "    #print(all_tweets)\n",
        "    counter_words = Counter(all_tweets)\n",
        "    #print(counter_words)\n",
        "    #print(len(counter_words))\n",
        "    counter_words = counter_words.most_common(n)\n",
        "    vocab_to_int = {w:i+1 for i, (w,c) in enumerate(counter_words)}\n",
        "    #print(vocab_to_int)\n",
        "    return vocab_to_int\n",
        "  \n",
        "def padding(dataset_tok, max_len):\n",
        "  for i in range(len(dataset_tok)):\n",
        "      if len(dataset_tok[i]) < max_len:\n",
        "      #if len(dataset_tok[i]) < max_len:\n",
        "          for z in range(max_len - len(dataset_tok[i])):\n",
        "          #for z in range(max_len - len(tweet)):\n",
        "              # dataset_tok[i].append([0])\n",
        "              dataset_tok[i] = [[0]] + dataset_tok[i]\n",
        "              #tweet.append(0)\n",
        "  return np.array(dataset_tok)\n",
        "\n",
        "def tweets_tok(dataset, vocab_to_int):\n",
        "\n",
        "    dataset_tok = []\n",
        "    max_len = 0\n",
        "\n",
        "    # for tweet in dataset_tok:\n",
        "    #   print(tweet)\n",
        "    #     r = [vocab_to_int[w] if w in vocab_to_int else 0 for w in tweet.split()]\n",
        "    #     dataset_tok[tweet]=r\n",
        "    #     #print(r)\n",
        "    #     #tweet = r\n",
        "    for i in range(len(dataset)):\n",
        "        r = [[vocab_to_int[w]] for w in dataset.iloc[i].split() if w in vocab_to_int]\n",
        "        dataset_tok.append(r)\n",
        "        #dataset_tok.iloc[i] = r\n",
        "\n",
        "        if len(r) > max_len:\n",
        "            max_len = len(r)\n",
        "        #print(r)\n",
        "        #tweet = r\n",
        "    \n",
        "    # # padding\n",
        "    # for i in range(len(dataset_tok)):\n",
        "    #     #if len(dataset_tok.iloc[i]) < max_len:\n",
        "    #     if len(dataset_tok[i]) < max_len:\n",
        "    #         #for z in range(max_len - len(dataset_tok.iloc[i])):\n",
        "    #         for z in range(max_len - len(dataset_tok[i])):\n",
        "    #             #dataset_tok.iloc[i].append(0)\n",
        "    #             dataset_tok[i].append(0)\n",
        "\n",
        "    return dataset_tok, max_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RTfnfKR3XnBh",
        "outputId": "8d3e8d68-bf32-473c-ff16-0dfed3f14f08"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'train_x' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn [8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_x_clean \u001b[39m=\u001b[39m train_x\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x:separateEmoji(x))\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x:removeMention(x))\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x:removeLink(x))\n\u001b[0;32m      2\u001b[0m test_x_clean \u001b[39m=\u001b[39m covid_test[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x:separateEmoji(x))\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x:removeMention(x))\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x:removeLink(x))\n\u001b[0;32m      3\u001b[0m train_x_clean\n",
            "\u001b[1;31mNameError\u001b[0m: name 'train_x' is not defined"
          ]
        }
      ],
      "source": [
        "train_x_clean = train_x.apply(lambda x:separateEmoji(x)).apply(lambda x:removeMention(x)).apply(lambda x:removeLink(x))\n",
        "test_x_clean = covid_test['text'].apply(lambda x:separateEmoji(x)).apply(lambda x:removeMention(x)).apply(lambda x:removeLink(x))\n",
        "train_x_clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9BUIq_nwLBYe",
        "outputId": "d0bb3203-780e-4379-ea20-d8f82928a416"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'train_x_clean' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn [9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m vocab_to_int \u001b[39m=\u001b[39m tweets_vocab(train_x_clean)\n\u001b[0;32m      2\u001b[0m train_x_tok, max_len1 \u001b[39m=\u001b[39m tweets_tok(train_x_clean, vocab_to_int)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'train_x_clean' is not defined"
          ]
        }
      ],
      "source": [
        "vocab_to_int = tweets_vocab(train_x_clean)\n",
        "train_x_tok, max_len1 = tweets_tok(train_x_clean, vocab_to_int)\n",
        "#print(train_x_tok)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "-vAJV7SULW6V"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'test_x_clean' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn [10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test_x_tok, max_len2 \u001b[39m=\u001b[39m tweets_tok(test_x_clean, vocab_to_int)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'test_x_clean' is not defined"
          ]
        }
      ],
      "source": [
        "test_x_tok, max_len2 = tweets_tok(test_x_clean, vocab_to_int)\n",
        "#print(test_x_tok)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CprKEdSwYRUU"
      },
      "outputs": [],
      "source": [
        "print(test_x_tok[0])\n",
        "max_len = max_len2\n",
        "if max_len1 > max_len2:\n",
        "  max_len = max_len1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmM6oYxrK1bF",
        "outputId": "997951ed-0b90-4258-90eb-4b3c070fa4ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[[   0]\n",
            "  [   0]\n",
            "  [   0]\n",
            "  ...\n",
            "  [  14]\n",
            "  [ 606]\n",
            "  [ 308]]\n",
            "\n",
            " [[   0]\n",
            "  [   0]\n",
            "  [   0]\n",
            "  ...\n",
            "  [  16]\n",
            "  [ 468]\n",
            "  [  20]]\n",
            "\n",
            " [[   0]\n",
            "  [   0]\n",
            "  [   0]\n",
            "  ...\n",
            "  [1248]\n",
            "  [1249]\n",
            "  [ 687]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[   0]\n",
            "  [   0]\n",
            "  [   0]\n",
            "  ...\n",
            "  [ 181]\n",
            "  [ 761]\n",
            "  [1501]]\n",
            "\n",
            " [[   0]\n",
            "  [   0]\n",
            "  [   0]\n",
            "  ...\n",
            "  [ 132]\n",
            "  [ 109]\n",
            "  [1730]]\n",
            "\n",
            " [[   0]\n",
            "  [   0]\n",
            "  [   0]\n",
            "  ...\n",
            "  [ 493]\n",
            "  [ 115]\n",
            "  [  44]]]\n"
          ]
        }
      ],
      "source": [
        "train_x_tok = padding(train_x_tok, max_len)\n",
        "test_x_tok = padding(test_x_tok, max_len)\n",
        "#train_x_tok.head()\n",
        "print(train_x_tok)\n",
        "#print(test_x_tok)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4V1Rw_xVu73K",
        "outputId": "c26901f6-b784-480c-c85b-3fdb35bf737b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([1, 1, 1,  ..., 0, 0, 1])"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.from_numpy(train_y.to_numpy()).to(torch.int64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnJshClLvDvE",
        "outputId": "7c47cef3-b141-4212-e8f1-a94b1e91b151"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0       1.0\n",
            "1       1.0\n",
            "2       1.0\n",
            "3       2.0\n",
            "4       1.0\n",
            "       ... \n",
            "2283    0.0\n",
            "2286    0.0\n",
            "2287    0.0\n",
            "2288    0.0\n",
            "2289    1.0\n",
            "Name: label, Length: 1995, dtype: float64\n",
            "[1. 2. 0.]\n"
          ]
        }
      ],
      "source": [
        "print(train_y)\n",
        "print(train_y.unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzFKx8-IlHuG",
        "outputId": "d4c5f1c6-d867-45f5-8238-2b1fbe1f2485"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.0    752\n",
            "1.0    685\n",
            "0.0    558\n",
            "Name: label, dtype: int64\n",
            "1    161237\n",
            "2    129636\n",
            "0     85516\n",
            "Name: BERT_label, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(train_y.value_counts())\n",
        "print(covid_test['BERT_label'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xuTlncO2pcEF",
        "outputId": "43088712-7689-4eec-af6a-50e0aa15f2c3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "index         161237\n",
              "Tweet ID      161237\n",
              "text          161237\n",
              "BERT_label    161237\n",
              "dtype: int64"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "covid_test[covid_test['BERT_label']==1].count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22B1GNaFsMNe",
        "outputId": "2f8ef263-86a0-44ae-a990-19c96cdd7070"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0 1]\n",
            "[0 1]\n"
          ]
        }
      ],
      "source": [
        "train_y = train_y.mask(train_y < 2, 0).astype(int)\n",
        "train_y = train_y.mask(train_y == 2, 1).astype(int)\n",
        "\n",
        "test_y = covid_test['BERT_label']\n",
        "test_y = test_y.mask(test_y < 2, 0).astype(int)\n",
        "test_y = test_y.mask(test_y == 2, 1).astype(int)\n",
        "\n",
        "# test_y = covid_test['BERT_label'].mask(covid_test['BERT_label'] < 2, 0).astype(int)\n",
        "# test_y = covid_test['BERT_label'].mask(covid_test['BERT_label'] == 2, 1).astype(int)\n",
        "\n",
        "print(train_y.unique())\n",
        "print(test_y.unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjJqXqO-oD7j",
        "outputId": "d24700bc-03fb-4997-e101-a16ce101c2b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0    1243\n",
            "1     752\n",
            "Name: label, dtype: int64\n",
            "0    246753\n",
            "1    129636\n",
            "Name: BERT_label, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(train_y.value_counts())\n",
        "print(test_y.value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqSwNnIfH2aR",
        "outputId": "10e81f23-cfbd-4676-a722-81c4c0eb2d71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.series.Series'>\n",
            "<class 'numpy.int64'>\n"
          ]
        }
      ],
      "source": [
        "print(type(train_y[0]))\n",
        "print(type(test_y[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FaDlZ_Oxwoa4"
      },
      "outputs": [],
      "source": [
        "378384"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dUsJRqWsgzYx"
      },
      "outputs": [],
      "source": [
        "# train_x_comp = np.load('train_x.npy')\n",
        "train_y = np.load('train_y.npy')\n",
        "# val_x_tok = np.load('val_x.npy')\n",
        "val_y = np.load('val_y.npy')\n",
        "# test_x_tok = np.load('test_x.npy')\n",
        "test_y = np.load('test_y.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0         Oh you people are unbelievably stupid why the ...\n",
              "1         Kazumi: \"Not all Asian women have flat asses. ...\n",
              "2         BTS Wordle 220525 2/6 🟪 ⬛ ️ ⬛ ️ 🟪 ⬛ ️ 🟪 🟪 🟪 🟪 ...\n",
              "3         Theres all types of privilege u jus dnt recogn...\n",
              "4         Do you think Canberra, and the rest of the wor...\n",
              "                                ...                        \n",
              "303102    It was my Granddaughters 2nd birthday at the w...\n",
              "303103    I don't think that chart shows identical figur...\n",
              "303104         Delete this. It ain’t funny and it’s racist.\n",
              "303105                                       Not this again\n",
              "303106    Who needs 'em. DiSantis knows COVID is fake ne...\n",
              "Length: 303107, dtype: object"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_x_tok = pd.Series(np.load('train_words_x.npy', allow_pickle=True))\n",
        "# print(test_x_tok.head())\n",
        "train_x_tok = train_x_tok.apply(lambda x:separateEmoji(x)).apply(lambda x:removeMention(x)).apply(lambda x:removeLink(x))\n",
        "# vocab_to_int = tweets_vocab(train_x_tok, 200)\n",
        "# train_x_tok_2 = [i.split() for i in train_x_tok]\n",
        "train_x_tok\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['a', 'b', 'c'], ['a', 'b', 'c'], ['a', 'b']]\n"
          ]
        }
      ],
      "source": [
        "t = [['a', 'b', 'c', 'd'],['a', 'b', 'c'],['a', 'b', 'd']]\n",
        "u = [[word for word in l if word in(['a','b','c'])] for l in t]\n",
        "print(u)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "cntvct = CountVectorizer(max_features=2000)\n",
        "train_x_tok_3 = cntvct.fit_transform(train_x_tok)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "svd = TruncatedSVD(n_components=200, random_state=42)\n",
        "train_x_svd = svd.fit_transform(train_x_tok_3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_x_tok = pd.Series(np.load('test_words_x.npy', allow_pickle=True))\n",
        "# print(test_x_tok.head())\n",
        "test_x_tok = test_x_tok.apply(lambda x:separateEmoji(x)).apply(lambda x:removeMention(x)).apply(lambda x:removeLink(x))\n",
        "# test_x_tok = [i.split() for i in test_x_tok]\n",
        "\n",
        "val_x_tok = pd.Series(np.load('val_words_x.npy', allow_pickle=True))\n",
        "# print(test_x_tok.head())\n",
        "val_x_tok = val_x_tok.apply(lambda x:separateEmoji(x)).apply(lambda x:removeMention(x)).apply(lambda x:removeLink(x))\n",
        "# val_x_tok = [i.split() for i in val_x_tok]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "val_x_tok_3 = cntvct.transform(val_x_tok)\n",
        "test_x_tok_3 = cntvct.transform(test_x_tok)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "val_x_svd = svd.transform(val_x_tok_3)\n",
        "test_x_svd = svd.transform(test_x_tok_3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(37639, 200)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_x_svd.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[[ 2.19055459e+00],\n",
              "        [-1.29294448e+00],\n",
              "        [ 2.60391923e+00],\n",
              "        ...,\n",
              "        [ 2.22318661e-02],\n",
              "        [ 2.45931310e-04],\n",
              "        [ 3.25665555e-02]],\n",
              "\n",
              "       [[ 2.56397018e-01],\n",
              "        [-1.48115728e-01],\n",
              "        [ 2.48622158e-02],\n",
              "        ...,\n",
              "        [ 5.21197792e-02],\n",
              "        [-1.26416102e-01],\n",
              "        [ 4.05924299e-02]],\n",
              "\n",
              "       [[ 5.15309733e-01],\n",
              "        [-2.09957723e-02],\n",
              "        [-1.28170289e-01],\n",
              "        ...,\n",
              "        [-2.43712375e-02],\n",
              "        [-2.39855146e-02],\n",
              "        [ 8.02499397e-02]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[ 7.50929123e-01],\n",
              "        [-5.69521894e-01],\n",
              "        [ 1.42043563e-01],\n",
              "        ...,\n",
              "        [-3.77241034e-02],\n",
              "        [ 4.80523140e-03],\n",
              "        [-5.58832336e-03]],\n",
              "\n",
              "       [[ 1.93942152e-01],\n",
              "        [-1.38873489e-01],\n",
              "        [ 8.30667279e-03],\n",
              "        ...,\n",
              "        [ 9.30617252e-02],\n",
              "        [-2.98220194e-02],\n",
              "        [ 2.97058794e-02]],\n",
              "\n",
              "       [[ 1.66959434e+00],\n",
              "        [-4.82385266e-01],\n",
              "        [ 1.58670031e+00],\n",
              "        ...,\n",
              "        [ 1.20457715e-01],\n",
              "        [-1.23149464e-01],\n",
              "        [-3.76536713e-02]]])"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.expand_dims(train_x_svd, axis=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "aMgDKasuh1H7"
      },
      "outputs": [],
      "source": [
        "#train_1_ds = TensorDataset(torch.from_numpy(train_x_tok).type(torch.float32), torch.from_numpy(train_y.to_numpy()))\n",
        "#train_2_ds = TensorDataset(torch.from_numpy(train_x_tok_2).type(torch.float32), torch.from_numpy(train_y_2))\n",
        "train_ds = TensorDataset(torch.from_numpy(np.expand_dims(train_x_svd, axis=2)).type(torch.float32), torch.from_numpy(train_y))\n",
        "#train_ds = torch.utils.data.ConcatDataset([train_1_ds, train_2_ds])\n",
        "val_ds = TensorDataset(torch.from_numpy(np.expand_dims(val_x_svd, axis=2)).type(torch.float32), torch.from_numpy(val_y))\n",
        "test_ds = TensorDataset(torch.from_numpy(np.expand_dims(test_x_svd, axis=2)).type(torch.float32), torch.from_numpy(test_y))\n",
        "\n",
        "batch_size = 256\n",
        "train_dl = DataLoader(train_ds, shuffle=True, batch_size=batch_size)\n",
        "val_dl = DataLoader(val_ds, shuffle=True, batch_size=batch_size)\n",
        "test_dl = DataLoader(test_ds, shuffle=True, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "max_len=128\n",
        "vocab_size=1996"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNWO4FgSUjsW"
      },
      "source": [
        "# LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "joB1aqaTLeeC"
      },
      "outputs": [],
      "source": [
        "class TweetsLSTM(nn.Module):\n",
        "    def __init__(self,no_layers,hidden_dim,input_dim,drop_prob=0.5):\n",
        "        super(TweetsLSTM,self).__init__()\n",
        " \n",
        "        #self.output_dim = output_dim\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.no_layers = no_layers\n",
        "    \n",
        "        print(type(self.no_layers), type(self.input_dim), type(self.hidden_dim))\n",
        "        \n",
        "        # embedding and LSTM layers\n",
        "        #self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        \n",
        "        #lstm\n",
        "        self.lstm_layers = nn.LSTM(input_size=self.input_dim,hidden_size=self.hidden_dim,\n",
        "                           num_layers=no_layers, batch_first=True)\n",
        "        \n",
        "        \n",
        "        # dropout layer\n",
        "        self.dropout_layer = nn.Dropout(p=drop_prob, inplace=True)\n",
        "    \n",
        "        # linear and sigmoid layer\n",
        "        self.linear_layer = nn.Linear(self.hidden_dim, 1)\n",
        "        self.sigmoid_layer = nn.Sigmoid()\n",
        "\n",
        "    def forward(self,x,hidden):\n",
        "        print(\"x:\", x.shape, x.dtype)\n",
        "        x = x.to(torch.device('cpu'))\n",
        "        h = hidden[0].to(torch.device('cpu'))\n",
        "        print(h.device)\n",
        "        c = hidden[1].to(torch.device('cpu'))\n",
        "        batch_size, _seq1 , _seq2 = x.size()\n",
        "        print(batch_size)\n",
        "        \n",
        "        # h_1 = torch.zeros(self.no_layers, batch_size, self.hidden_dim, device=torch.device('cpu'))\n",
        "        # c_1 = torch.zeros(self.no_layers, batch_size, self.hidden_dim, device=torch.device('cpu'))\n",
        "        \n",
        "        # hc_1 = (h_1, c_1)\n",
        "        \n",
        "        print(\"hidden:\", hidden[0].device, x.device)\n",
        "        # embeddings and lstm_out\n",
        "        #embeds = self.embedding(x)  # shape: B x S x Feature   since batch = True\n",
        "        #print(\"emb:\",embeds.shape)  #[50, 500, 1000]\n",
        "        #print(embeds[0])\n",
        "        #print(\"emb: \",embeds)\n",
        "\n",
        "        lstm_out, (h,c) = self.lstm_layers(x, (h, c))\n",
        "        print(\"lstm out:\",lstm_out.shape, hidden.device)\n",
        "        #lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim) \n",
        "        \n",
        "        # dropout and fully connected layer\n",
        "        out = self.dropout_layer(h) #(h_1) lstm_out\n",
        "        #print(out.contiguous().view(batch_size,-1))\n",
        "        #out = self.fc(out.contiguous().view(batch_size,-1))\n",
        "        #print(\"out:\", out,out.shape)\n",
        "        lin_out = self.linear_layer(out)\n",
        "        #print(\"lin_out: \", lin_out.shape)\n",
        "        \n",
        "        \n",
        "        # sigmoid function\n",
        "        sig_out = self.sigmoid_layer(lin_out)\n",
        "        #print(\"out2:\", sig_out.shape)\n",
        "        # reshape to be batch_size first\n",
        "        #sig_out = sig_out.view(batch_size, -1)\n",
        "        #print(\"sig_out: \", sig_out.shape)\n",
        "\n",
        "        #sig_out = sig_out[:, -1]\n",
        "        #print(sig_out)\n",
        "        # # reshape to be batch_size first\n",
        "        #sig_out = out.view(batch_size, -1)\n",
        "        #print(\"out3:\", sig_out.shape)\n",
        "        #sig_out = sig_out[:, -1] # get last batch of labels\n",
        "        #print(\"sig_out:\", sig_out.shape)\n",
        "        \n",
        "        #sig_out = out.view(batch_size,-1)\n",
        "        sig_out = sig_out[-1,:,:]\n",
        "        #print(sig_out.shape,sig_out)\n",
        "        # # return last sigmoid output and hidden state\n",
        "\n",
        "        #sig_out = torch.max(out,1)\n",
        "        # print(\"sig_out:\",sig_out.shape, sig_out)\n",
        "        #sig_out = torch.max(sig_out,1)\n",
        "        #print(\"sig_out:\",sig_out)\n",
        "        #return sig_out[1], hidden\n",
        "        return sig_out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        h0 = torch.zeros(self.no_layers,batch_size,self.hidden_dim,device=torch.device('cpu'))\n",
        "        c0 = torch.zeros(self.no_layers,batch_size,self.hidden_dim,device=torch.device('cpu'))\n",
        "        hidden = (h0,c0)\n",
        "        return hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "4LxyorjhPRys",
        "outputId": "fe480e4c-22ae-4b21-f3a0-845b05943075"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'int'> <class 'int'> <class 'int'>\n",
            "sample input:  torch.Size([256, 200, 1])\n",
            "sample output:  torch.Size([256])\n",
            "TweetsLSTM(\n",
            "  (lstm_layers): LSTM(1, 50, num_layers=2, batch_first=True)\n",
            "  (dropout_layer): Dropout(p=0.3, inplace=True)\n",
            "  (linear_layer): Linear(in_features=50, out_features=1, bias=True)\n",
            "  (sigmoid_layer): Sigmoid()\n",
            ")\n",
            "x: torch.Size([4, 200, 1]) torch.float32\n",
            "cpu\n",
            "4\n",
            "hidden: cuda:0 cpu\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Failed to run torchinfo. See above stack traces for more details. Executed layers up to: []",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Users\\sandr\\anaconda3\\envs\\tccenv\\lib\\site-packages\\torchinfo\\torchinfo.py:290\u001b[0m, in \u001b[0;36mforward_pass\u001b[1;34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(x, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[1;32m--> 290\u001b[0m     _ \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mto(device)(\u001b[39m*\u001b[39mx, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    291\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(x, \u001b[39mdict\u001b[39m):\n",
            "File \u001b[1;32mc:\\Users\\sandr\\anaconda3\\envs\\tccenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1148\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1146\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(\u001b[39minput\u001b[39m)\n\u001b[1;32m-> 1148\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1149\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n",
            "\u001b[1;32mc:\\Users\\sandr\\Documents\\poli-usp\\ano6\\tcc\\lstm_embedding.ipynb Célula: 42\u001b[0m in \u001b[0;36mTweetsLSTM.forward\u001b[1;34m(self, x, hidden)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sandr/Documents/poli-usp/ano6/tcc/lstm_embedding.ipynb#X55sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39m# embeddings and lstm_out\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sandr/Documents/poli-usp/ano6/tcc/lstm_embedding.ipynb#X55sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39m#embeds = self.embedding(x)  # shape: B x S x Feature   since batch = True\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sandr/Documents/poli-usp/ano6/tcc/lstm_embedding.ipynb#X55sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39m#print(\"emb:\",embeds.shape)  #[50, 500, 1000]\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sandr/Documents/poli-usp/ano6/tcc/lstm_embedding.ipynb#X55sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39m#print(embeds[0])\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sandr/Documents/poli-usp/ano6/tcc/lstm_embedding.ipynb#X55sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39m#print(\"emb: \",embeds)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/sandr/Documents/poli-usp/ano6/tcc/lstm_embedding.ipynb#X55sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m lstm_out, (h,c) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm_layers(x, (h, c))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sandr/Documents/poli-usp/ano6/tcc/lstm_embedding.ipynb#X55sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mlstm out:\u001b[39m\u001b[39m\"\u001b[39m,lstm_out\u001b[39m.\u001b[39mshape, hidden\u001b[39m.\u001b[39mdevice)\n",
            "File \u001b[1;32mc:\\Users\\sandr\\anaconda3\\envs\\tccenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1148\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1146\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(\u001b[39minput\u001b[39m)\n\u001b[1;32m-> 1148\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1149\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n",
            "File \u001b[1;32mc:\\Users\\sandr\\anaconda3\\envs\\tccenv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:769\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    768\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[0;32m    770\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[0;32m    771\u001b[0m \u001b[39melse\u001b[39;00m:\n",
            "\u001b[1;31mRuntimeError\u001b[0m: Input and parameter tensors are not at the same device, found input tensor at cpu and parameter tensor at cuda:0",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\sandr\\Documents\\poli-usp\\ano6\\tcc\\lstm_embedding.ipynb Célula: 42\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sandr/Documents/poli-usp/ano6/tcc/lstm_embedding.ipynb#X55sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m#summary(model, (4, 200, 1),dtypes=[torch.int])\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sandr/Documents/poli-usp/ano6/tcc/lstm_embedding.ipynb#X55sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m dev \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/sandr/Documents/poli-usp/ano6/tcc/lstm_embedding.ipynb#X55sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m summary(model, input_data\u001b[39m=\u001b[39;49m[torch\u001b[39m.\u001b[39;49mtensor(np\u001b[39m.\u001b[39;49mones((\u001b[39m4\u001b[39;49m, \u001b[39m200\u001b[39;49m, \u001b[39m1\u001b[39;49m)))\u001b[39m.\u001b[39;49mtype(torch\u001b[39m.\u001b[39;49mfloat32)\u001b[39m.\u001b[39;49mto(dev), (torch\u001b[39m.\u001b[39;49mtensor(np\u001b[39m.\u001b[39;49mzeros((\u001b[39m2\u001b[39;49m, \u001b[39m4\u001b[39;49m, \u001b[39m50\u001b[39;49m)))\u001b[39m.\u001b[39;49mto(dev), torch\u001b[39m.\u001b[39;49mtensor(np\u001b[39m.\u001b[39;49mzeros((\u001b[39m2\u001b[39;49m, \u001b[39m4\u001b[39;49m, \u001b[39m50\u001b[39;49m)))\u001b[39m.\u001b[39;49mto(dev))])\n",
            "File \u001b[1;32mc:\\Users\\sandr\\anaconda3\\envs\\tccenv\\lib\\site-packages\\torchinfo\\torchinfo.py:218\u001b[0m, in \u001b[0;36msummary\u001b[1;34m(model, input_size, input_data, batch_dim, cache_forward_pass, col_names, col_width, depth, device, dtypes, mode, row_settings, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    211\u001b[0m validate_user_params(\n\u001b[0;32m    212\u001b[0m     input_data, input_size, columns, col_width, device, dtypes, verbose\n\u001b[0;32m    213\u001b[0m )\n\u001b[0;32m    215\u001b[0m x, correct_input_size \u001b[39m=\u001b[39m process_input(\n\u001b[0;32m    216\u001b[0m     input_data, input_size, batch_dim, device, dtypes\n\u001b[0;32m    217\u001b[0m )\n\u001b[1;32m--> 218\u001b[0m summary_list \u001b[39m=\u001b[39m forward_pass(\n\u001b[0;32m    219\u001b[0m     model, x, batch_dim, cache_forward_pass, device, model_mode, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    220\u001b[0m )\n\u001b[0;32m    221\u001b[0m formatting \u001b[39m=\u001b[39m FormattingOptions(depth, verbose, columns, col_width, rows)\n\u001b[0;32m    222\u001b[0m results \u001b[39m=\u001b[39m ModelStatistics(\n\u001b[0;32m    223\u001b[0m     summary_list, correct_input_size, get_total_memory_used(x), formatting\n\u001b[0;32m    224\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\sandr\\anaconda3\\envs\\tccenv\\lib\\site-packages\\torchinfo\\torchinfo.py:299\u001b[0m, in \u001b[0;36mforward_pass\u001b[1;34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    298\u001b[0m     executed_layers \u001b[39m=\u001b[39m [layer \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m summary_list \u001b[39mif\u001b[39;00m layer\u001b[39m.\u001b[39mexecuted]\n\u001b[1;32m--> 299\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    300\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFailed to run torchinfo. See above stack traces for more details. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    301\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExecuted layers up to: \u001b[39m\u001b[39m{\u001b[39;00mexecuted_layers\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    302\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m    303\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    304\u001b[0m     \u001b[39mif\u001b[39;00m hooks:\n",
            "\u001b[1;31mRuntimeError\u001b[0m: Failed to run torchinfo. See above stack traces for more details. Executed layers up to: []"
          ]
        }
      ],
      "source": [
        "no_layers = 2\n",
        "#vocab_size = len(vocab_to_int) + 1 #extra 1 for padding\n",
        "#vocab_size = 128\n",
        "embedding_dim = 64\n",
        "input_dim = 1#200\n",
        "output_dim = 3\n",
        "hidden_dim = 50 #256\n",
        "\n",
        "\n",
        "model = TweetsLSTM(no_layers,hidden_dim,input_dim,drop_prob=0.3).to(torch.device('cpu'))\n",
        "\n",
        "#print(train_x_tok.shape)\n",
        "dataiter = iter(train_dl)\n",
        "print('sample input: ', dataiter.next()[0].size())\n",
        "print('sample output: ', dataiter.next()[1].size())\n",
        "print(model)\n",
        "#summary(model, (4, 200, 1),dtypes=[torch.int])\n",
        "dev = torch.device('cpu')\n",
        "summary(model, input_data=[torch.tensor(np.ones((4, 200, 1))).type(torch.float32).to(dev), (torch.tensor(np.zeros((2, 4, 50))).to(dev), torch.tensor(np.zeros((2, 4, 50))).to(dev))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "00M0z3b8PRnC"
      },
      "outputs": [],
      "source": [
        "# loss and optimization functions\n",
        "lr=0.001\n",
        "\n",
        "def BCELoss_class_weighted(weights):\n",
        "  def loss(input, target):\n",
        "    input = torch.clamp(input,min=1e-7,max=1-1e-7)\n",
        "    bce = - weights[1] * target * torch.log(input) - (1 - target) * weights[0] * torch.log(1 - input)\n",
        "    return torch.mean(bce)\n",
        "  return loss\n",
        "\n",
        "weights = [len(train_y)/(2*(len(train_y)-train_y.sum())), len(train_y)/(2*train_y.sum())]\n",
        "criterion = BCELoss_class_weighted(weights)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "# function to predict accuracy\n",
        "def acc(pred,label):\n",
        "    #pred = torch.round(pred.squeeze())\n",
        "    #print(pred, label, torch.sum(pred == label.squeeze()).item())\n",
        "    return torch.sum(((pred >= 0.5).type(torch.float32) == label.squeeze()).type(torch.float32)).item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLzeiv1fA81w",
        "outputId": "72063c5c-6f88-4008-e5f8-8620b19b168b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3.0"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a = torch.tensor(np.array([1, 0.6, 0.3, 0]))\n",
        "b = torch.tensor(np.array([1, 0, 0, 0]))\n",
        "torch.sum(((a >= 0.5).type(torch.int) == b).type(torch.float32)).item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "hfX04BCA_VY9"
      },
      "outputs": [],
      "source": [
        "def training_loop(dataloader, model, loss_fn, optimizer):\n",
        "  size = len(dataloader.dataset)\n",
        "  running_loss = 0\n",
        "\n",
        "  model.train()\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    optimizer.zero_grad()\n",
        "    pred = model(X)\n",
        "    #final_pred = torch.squeeze((pred >= 0.5).type(torch.int),1)\n",
        "    final_pred = torch.squeeze(pred, 1).type(torch.float32)\n",
        "    #y = torch.unsqueeze(y, 0)\n",
        "    #print(final_pred, y)\n",
        "    loss = loss_fn(final_pred, y.type(torch.float32))\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss = running_loss + loss.item()\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      loss, current = loss.item(), batch*len(X)\n",
        "      print(f'loss: {loss:>7f}  [{current:>5d}/{size:>5d}]')\n",
        "    \n",
        "  return running_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLtUQVBIBEu7"
      },
      "outputs": [],
      "source": [
        "def testing_loop(dataloader, model, loss_fn):\n",
        "  size = len(dataloader.dataset)\n",
        "  num_batches = len(dataloader)\n",
        "  test_loss, correct = 0, 0\n",
        "  running_loss = 0\n",
        "\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for X, y in dataloader:\n",
        "      pred = model(X)\n",
        "      #final_pred = (pred >= 0.5).type(torch.int)\n",
        "      final_pred = torch.squeeze(pred, 1).type(torch.float32)\n",
        "      test_loss += loss_fn(final_pred, y.type(torch.float32)).item()\n",
        "      correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "      running_loss = running_loss + test_loss\n",
        "  \n",
        "  test_loss /= num_batches\n",
        "  correct /= size\n",
        "  print(f'Test Error: \\n Accuracy:{(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}\\n')\n",
        "\n",
        "  return running_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vmqohwXdI8wf",
        "outputId": "6f341660-c6e1-4c15-9d36-32396da41b07"
      },
      "outputs": [],
      "source": [
        "epochs = 50\n",
        "best_val_loss = 100000000\n",
        "for i in range (epochs):\n",
        "  tr_loss = training_loop(train_dl, model, criterion, optimizer)\n",
        "  val_loss = testing_loop(val_dl, model, criterion)\n",
        "  if val_loss < best_val_loss:\n",
        "    best_val_loss = val_loss\n",
        "    torch.save(model.state_dict(), '/content/drive/MyDrive/poli 2022/new_model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJzL2_26NXFW",
        "outputId": "56f79304-be5d-43bd-9810-5cf7a92d66e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "303107\n",
            "37638\n",
            "epoch: 0\n",
            "cpu cpu\n",
            "ok\n",
            "x: torch.Size([256, 200, 1]) torch.float32\n",
            "256\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Input and parameter tensors are not at the same device, found input tensor at cpu and parameter tensor at cuda:0",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\sandr\\Documents\\poli-usp\\ano6\\tcc\\lstm_embedding.ipynb Célula: 48\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sandr/Documents/poli-usp/ano6/tcc/lstm_embedding.ipynb#X64sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sandr/Documents/poli-usp/ano6/tcc/lstm_embedding.ipynb#X64sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mok\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/sandr/Documents/poli-usp/ano6/tcc/lstm_embedding.ipynb#X64sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m output, h \u001b[39m=\u001b[39m model(inputs, h)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sandr/Documents/poli-usp/ano6/tcc/lstm_embedding.ipynb#X64sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# calculate the loss and perform backprop\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sandr/Documents/poli-usp/ano6/tcc/lstm_embedding.ipynb#X64sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m#print(\"output:\",output.shape, output[0].shape, output[0].dtype)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sandr/Documents/poli-usp/ano6/tcc/lstm_embedding.ipynb#X64sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m#print(\"labels:\",labels.shape, labels.dtype)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sandr/Documents/poli-usp/ano6/tcc/lstm_embedding.ipynb#X64sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39m#print(output, torch.max(labels,1)[1])\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sandr/Documents/poli-usp/ano6/tcc/lstm_embedding.ipynb#X64sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m#print(output.T, labels.type(torch.float32))\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sandr/Documents/poli-usp/ano6/tcc/lstm_embedding.ipynb#X64sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(torch\u001b[39m.\u001b[39msqueeze(output, \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mtype(torch\u001b[39m.\u001b[39mfloat32), labels\u001b[39m.\u001b[39mtype(torch\u001b[39m.\u001b[39mfloat32))\n",
            "File \u001b[1;32mc:\\Users\\sandr\\anaconda3\\envs\\tccenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "\u001b[1;32mc:\\Users\\sandr\\Documents\\poli-usp\\ano6\\tcc\\lstm_embedding.ipynb Célula: 48\u001b[0m in \u001b[0;36mTweetsLSTM.forward\u001b[1;34m(self, x, hidden)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sandr/Documents/poli-usp/ano6/tcc/lstm_embedding.ipynb#X64sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mprint\u001b[39m(batch_size)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sandr/Documents/poli-usp/ano6/tcc/lstm_embedding.ipynb#X64sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39m# h_1 = torch.zeros(self.no_layers, batch_size, self.hidden_dim, device=torch.device('cpu'))\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sandr/Documents/poli-usp/ano6/tcc/lstm_embedding.ipynb#X64sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39m# c_1 = torch.zeros(self.no_layers, batch_size, self.hidden_dim, device=torch.device('cpu'))\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sandr/Documents/poli-usp/ano6/tcc/lstm_embedding.ipynb#X64sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sandr/Documents/poli-usp/ano6/tcc/lstm_embedding.ipynb#X64sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39m#print(embeds[0])\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sandr/Documents/poli-usp/ano6/tcc/lstm_embedding.ipynb#X64sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39m#print(\"emb: \",embeds)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/sandr/Documents/poli-usp/ano6/tcc/lstm_embedding.ipynb#X64sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m lstm_out, hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm_layers(x, hidden)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sandr/Documents/poli-usp/ano6/tcc/lstm_embedding.ipynb#X64sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mlstm out:\u001b[39m\u001b[39m\"\u001b[39m,lstm_out\u001b[39m.\u001b[39mshape, hidden\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sandr/Documents/poli-usp/ano6/tcc/lstm_embedding.ipynb#X64sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39m#lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim) \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sandr/Documents/poli-usp/ano6/tcc/lstm_embedding.ipynb#X64sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sandr/Documents/poli-usp/ano6/tcc/lstm_embedding.ipynb#X64sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39m# dropout and fully connected layer\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\sandr\\anaconda3\\envs\\tccenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\sandr\\anaconda3\\envs\\tccenv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:769\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    767\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_forward_args(\u001b[39minput\u001b[39m, hx, batch_sizes)\n\u001b[0;32m    768\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[0;32m    770\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[0;32m    771\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    772\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[0;32m    773\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n",
            "\u001b[1;31mRuntimeError\u001b[0m: Input and parameter tensors are not at the same device, found input tensor at cpu and parameter tensor at cuda:0"
          ]
        }
      ],
      "source": [
        "clip = 5\n",
        "epochs = 50\n",
        "valid_loss_min = np.Inf\n",
        "# train for some number of epochs\n",
        "epoch_tr_loss,epoch_vl_loss = [],[]\n",
        "epoch_tr_acc,epoch_vl_acc = [],[]\n",
        "print(len(train_dl.dataset))\n",
        "print(len(val_dl.dataset))\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(\"epoch:\", epoch)\n",
        "    train_losses = []\n",
        "    train_acc = 0.0\n",
        "    model.train()\n",
        "    # initialize hidden state \n",
        "    h = model.init_hidden(batch_size)\n",
        "    for inputs, labels in train_dl:\n",
        "        print(inputs.device, h[0].device)\n",
        "        \n",
        "        # Creating new variables for the hidden state, otherwise\n",
        "        # we'd backprop through the entire training history\n",
        "        h = tuple([each.data for each in h])\n",
        "        optimizer.zero_grad()\n",
        "        print('ok')\n",
        "        output, h = model(inputs, h)\n",
        "        \n",
        "        # calculate the loss and perform backprop\n",
        "        #print(\"output:\",output.shape, output[0].shape, output[0].dtype)\n",
        "        #print(\"labels:\",labels.shape, labels.dtype)\n",
        "        #print(output, torch.max(labels,1)[1])\n",
        "        #print(output.T, labels.type(torch.float32))\n",
        "        loss = criterion(torch.squeeze(output, 1).type(torch.float32), labels.type(torch.float32))\n",
        "        loss.backward()\n",
        "        train_losses.append(loss.item())\n",
        "        # calculating accuracy\n",
        "        #accuracy = acc(output,labels)\n",
        "        accuracy = acc(torch.squeeze(output, 1).type(torch.float32), labels.type(torch.float32))\n",
        "        train_acc += accuracy\n",
        "        #`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        #nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "    \n",
        "    val_h = model.init_hidden(batch_size)\n",
        "    val_losses = []\n",
        "    val_acc = 0.0\n",
        "    model.eval()\n",
        "    for inputs, labels in val_dl:\n",
        "            print(inputs.device, val_h.device)\n",
        "            val_h = tuple([each.data for each in val_h])\n",
        "            output, val_h = model(inputs, val_h)\n",
        "            #print(\"output:\",output.squeeze())\n",
        "            #print(\"labels:\",labels.long())\n",
        "            val_loss = criterion(torch.squeeze(output, 1).type(torch.float32), labels.type(torch.float32))\n",
        "\n",
        "            val_losses.append(val_loss.item())\n",
        "            \n",
        "            accuracy = acc(torch.squeeze(output, 1).type(torch.float32), labels.type(torch.float32))\n",
        "            val_acc += accuracy\n",
        "            epoch_train_loss = np.mean(train_losses)\n",
        "    epoch_val_loss = np.mean(val_losses)\n",
        "    epoch_train_acc = train_acc/len(train_dl.dataset)\n",
        "    epoch_val_acc = val_acc/len(val_dl.dataset)\n",
        "    epoch_tr_loss.append(epoch_train_loss)\n",
        "    epoch_vl_loss.append(epoch_val_loss)\n",
        "    epoch_tr_acc.append(epoch_train_acc)\n",
        "    epoch_vl_acc.append(epoch_val_acc)\n",
        "    print(f'Epoch {epoch+1}') \n",
        "    print(f'train_loss : {epoch_train_loss} val_loss : {epoch_val_loss}')\n",
        "    print(f'train_accuracy : {epoch_train_acc*100} val_accuracy : {epoch_val_acc*100}')\n",
        "    if epoch_val_loss <= valid_loss_min:\n",
        "        torch.save(model.state_dict(), 'model_emb')\n",
        "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,epoch_val_loss))\n",
        "        valid_loss_min = epoch_val_loss\n",
        "    print(25*'==')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlXKxe2hxNKz"
      },
      "outputs": [],
      "source": [
        "model = TweetsLSTM(no_layers,hidden_dim,input_dim,drop_prob=0.5)\n",
        "model.load('/content/drive/MyDrive/poli 2022/model')\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ux33WJw7ZPyD",
        "outputId": "4b8ebde2-2293-4be0-f32d-643a05982e8b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([], shape=(1, 0), dtype=float64)"
            ]
          },
          "execution_count": 124,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.empty((1,0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "TklfNz7-NW3m",
        "outputId": "c7f5ff92-9e4a-495b-8ced-917612a3e543"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.7760301814607189\n",
            "(37639,) [1. 0. 1. ... 1. 1. 0.]\n",
            "(37639,) [1. 0. 0. ... 0. 1. 0.]\n",
            "f1:  0.704625087596356\n",
            "recall:  0.778250773993808\n",
            "precision:  0.6437259923175416\n",
            "accuracy:  0.7760301814607189\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score\n",
        "\n",
        "def predict_text(inputs):\n",
        "      output = model(inputs)\n",
        "      return output\n",
        "\n",
        "corrects = 0\n",
        "total = 0\n",
        "TP = 0\n",
        "TN = 0\n",
        "FP = 0\n",
        "FN = 0\n",
        "all_pred_bin = np.empty((1,0))\n",
        "all_labels = np.empty((1,0))\n",
        "\n",
        "for inputs, labels in test_dl:\n",
        "  #print(inputs.shape)\n",
        "  pred_test = predict_text(inputs)\n",
        "  #print(torch.squeeze(pred_test, 1).type(torch.float32))\n",
        "  all_pred_bin = np.append(all_pred_bin, (torch.squeeze(pred_test, 1).type(torch.float32) >= 0.5).type(torch.int).detach().numpy())\n",
        "  all_labels = np.append(all_labels, labels.numpy())\n",
        "  #print(torch.squeeze(pred_test.T, 0), labels.type(torch.float32))\n",
        "  corrects += acc(torch.squeeze(pred_test, 1).type(torch.float32), labels.type(torch.float32))#acc(pred_test, labels)\n",
        "  #print(corrects)\n",
        "  total += len(pred_test)\n",
        "print(corrects/total)\n",
        "print(all_pred_bin.shape, all_pred_bin)\n",
        "print(all_labels.shape, all_labels)\n",
        "print(\"f1: \", f1_score(all_labels, all_pred_bin))\n",
        "print(\"recall: \", recall_score(all_labels, all_pred_bin))\n",
        "print(\"precision: \", precision_score(all_labels, all_pred_bin))\n",
        "print(\"accuracy: \", accuracy_score(all_labels, all_pred_bin))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQHo4vGMrkTq",
        "outputId": "317d2511-43d5-456f-a78c-0a82d804fb2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "37639.0\n"
          ]
        }
      ],
      "source": [
        "print(all_pred_bin.sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07PgfvuKrmfs"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('tccenv')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "e92cc3d26c38c253be3220a870a337609e115e6bcf47fea649f85669fc5c0a76"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
